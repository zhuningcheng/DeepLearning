{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "999c8f0f-5ba3-409c-9f65-5647ada32378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里使用codebert实现了java到python的代码翻译功能使用的是预训练codebert模型\n",
    "#模型是出自于微软的microsoft/codebert-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bb62f22-6564-4cbc-a76f-5f31c30fe8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "325b763d-af22-4957-96e2-bc45445287f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 任务3：数据集类定义\n",
    "# 运行这个单元格定义数据加载器\n",
    "\n",
    "class CodeTranslationDataset(Dataset):\n",
    "    def __init__(self, source_codes, target_codes, tokenizer, max_length=64):\n",
    "        self.source_codes = source_codes\n",
    "        self.target_codes = target_codes\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.source_codes)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        source_code = str(self.source_codes[idx])\n",
    "        target_code = str(self.target_codes[idx])\n",
    "        \n",
    "        # Tokenize source code\n",
    "        source_encoding = self.tokenizer(\n",
    "            source_code,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Tokenize target code\n",
    "        target_encoding = self.tokenizer(\n",
    "            target_code,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': source_encoding['input_ids'].flatten(),\n",
    "            'attention_mask': source_encoding['attention_mask'].flatten(),\n",
    "            'labels': target_encoding['input_ids'].flatten()\n",
    "        }\n",
    "\n",
    "def load_data(file_path):\n",
    "    \n",
    "    df = pd.read_csv(file_path)\n",
    "    return df['source_code'].tolist(), df['target_code'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed07052f-dc26-42af-a75a-2ab91e406f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建CodeBert的模型\n",
    "\n",
    "class ImprovedCodeBERTTranslator(nn.Module):\n",
    "    def __init__(self, model_name='microsoft/codebert-base', vocab_size=50265, max_length=64):\n",
    "        super(ImprovedCodeBERTTranslator, self).__init__()\n",
    "        \n",
    "        # 加载预训练的CodeBERT模型\n",
    "        self.codebert = RobertaModel.from_pretrained(model_name)\n",
    "        self.hidden_size = self.codebert.config.hidden_size\n",
    "        \n",
    "        # 解码器 \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.hidden_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, vocab_size)\n",
    "        )\n",
    "        \n",
    "        print(f\"模型初始化完成: hidden_size={self.hidden_size}, vocab_size={vocab_size}\")\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        # 编码器前向传播\n",
    "        encoder_outputs = self.codebert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "        \n",
    "        # 使用[CLS] token的输出作为序列表示\n",
    "        cls_output = encoder_outputs.last_hidden_state[:, 0, :]\n",
    "        \n",
    "        # 解码\n",
    "        logits = self.decoder(cls_output)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a0e15df-8bca-445b-bc1c-3fb341a82910",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, epochs=10, learning_rate=5e-5):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    \n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=1)  # 忽略padding token\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    print(f\"训练样本数: {len(train_loader.dataset)}\")\n",
    "    print(f\"验证样本数: {len(val_loader.dataset)}\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        # 训练阶段\n",
    "        train_pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs} [Train]')\n",
    "        for batch_idx, batch in enumerate(train_pbar):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask, labels)\n",
    "            \n",
    "            # 计算准确率 - 预测目标序列的第一个token\n",
    "            preds = torch.argmax(outputs, dim=-1)\n",
    "            targets = labels[:, 0]  # 目标序列的第一个token\n",
    "            \n",
    "            correct = (preds == targets).sum().item()\n",
    "            total = targets.size(0)\n",
    "            train_correct += correct\n",
    "            train_total += total\n",
    "            \n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_train_loss += loss.item()\n",
    "            train_pbar.set_postfix({\n",
    "                'loss': f'{loss.item():.4f}',\n",
    "                'acc': f'{correct/total:.4f}' if total > 0 else '0.0000'\n",
    "            })\n",
    "        \n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        train_accuracy = train_correct / train_total if train_total > 0 else 0\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        # 验证阶段\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            val_pbar = tqdm(val_loader, desc=f'Epoch {epoch+1}/{epochs} [Val]')\n",
    "            for batch in val_pbar:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                \n",
    "                outputs = model(input_ids, attention_mask, labels)\n",
    "                preds = torch.argmax(outputs, dim=-1)\n",
    "                targets = labels[:, 0]\n",
    "                \n",
    "                correct = (preds == targets).sum().item()\n",
    "                total = targets.size(0)\n",
    "                val_correct += correct\n",
    "                val_total += total\n",
    "                \n",
    "                loss = criterion(outputs, targets)\n",
    "                total_val_loss += loss.item()\n",
    "                \n",
    "                val_pbar.set_postfix({\n",
    "                    'loss': f'{loss.item():.4f}',\n",
    "                    'acc': f'{correct/total:.4f}' if total > 0 else '0.0000'\n",
    "                })\n",
    "        \n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        val_accuracy = val_correct / val_total if val_total > 0 else 0\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}: Train Loss: {avg_train_loss:.4f}, Train Acc: {train_accuracy:.4f}, Val Loss: {avg_val_loss:.4f}, Val Acc: {val_accuracy:.4f}')\n",
    "    \n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c066ef5e-f4d8-47f1-872f-307d4ff9bfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_code(model, tokenizer, source_code, device):\n",
    "        model.eval()\n",
    "        \n",
    "        # 编码源代码\n",
    "        encoding = tokenizer(\n",
    "            source_code,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=64,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        input_ids = encoding['input_ids'].to(device)\n",
    "        attention_mask = encoding['attention_mask'].to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            \n",
    "            # 获取预测结果\n",
    "            predicted_token_id = torch.argmax(outputs, dim=-1).item()\n",
    "            \n",
    "            # 解码预测的token\n",
    "            predicted_token = tokenizer.decode([predicted_token_id], skip_special_tokens=True)\n",
    "            \n",
    "            # 获取top-k预测结果\n",
    "            top_k = 5\n",
    "            probs = torch.softmax(outputs, dim=-1)\n",
    "            top_probs, top_indices = torch.topk(probs, top_k)\n",
    "            \n",
    "            print(f\"Top-{top_k} 预测:\")\n",
    "            for i in range(top_k):\n",
    "                token_id = top_indices[0, i].item()\n",
    "                token = tokenizer.decode([token_id], skip_special_tokens=True)\n",
    "                prob = top_probs[0, i].item()\n",
    "                print(f\"  {i+1}. '{token}' (ID: {token_id}, 概率: {prob:.4f})\")\n",
    "        \n",
    "        print(f\"最终翻译结果: '{predicted_token}'\")\n",
    "        return predicted_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38ea84c9-d5d3-4c09-832c-91ed9a006154",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, tokenizer, test_samples, device):\n",
    "    results = []\n",
    "    \n",
    "    \n",
    "    for i, (source, target) in enumerate(test_samples):\n",
    "        print(f\"\\n--- 测试样本 {i+1}/{len(test_samples)} ---\")\n",
    "        translated = translate_code(model, tokenizer, source, device)\n",
    "        \n",
    "        # 检查翻译结果\n",
    "        exact_match = translated.strip() == target.strip()\n",
    "        \n",
    "        results.append({\n",
    "            'source': source,\n",
    "            'target': target,\n",
    "            'translated': translated,\n",
    "            'exact_match': exact_match\n",
    "        })\n",
    "        \n",
    "        print(f\"期望结果: '{target}'\")\n",
    "        print(f\"是否匹配: {exact_match}\")\n",
    "    \n",
    "    exact_match_accuracy = sum([r['exact_match'] for r in results]) / len(results)\n",
    "    print(f\"\\n精确匹配准确率: {exact_match_accuracy:.4f} ({sum([r['exact_match'] for r in results])}/{len(results)})\")\n",
    "    \n",
    "    return results, exact_match_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8f26eec-b329-4690-9139-990e63b1c612",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # 配置参数\n",
    "    config = {\n",
    "        'batch_size': 4,\n",
    "        'max_length': 64,  # 序列长度\n",
    "        'learning_rate': 3e-5,  # 学习率\n",
    "        'epochs': 15,      # 训练轮数\n",
    "        'model_name': 'microsoft/codebert-base'\n",
    "    }\n",
    "    \n",
    "    print(f\"配置参数: {config}\")\n",
    "    \n",
    "    # 加载tokenizer\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(config['model_name'])\n",
    "    print(f\"Tokenizer词汇表大小: {tokenizer.vocab_size}\")\n",
    "    \n",
    "    # 加载数据\n",
    "    train_source, train_target = load_data('data/train.csv')\n",
    "    val_source, val_target = load_data('data/val.csv')\n",
    "    test_source, test_target = load_data('data/test.csv')\n",
    "    \n",
    "    print(f\"数据集大小:\")\n",
    "    print(f\"- 训练集: {len(train_source)} 个样本\")\n",
    "    print(f\"- 验证集: {len(val_source)} 个样本\")\n",
    "    print(f\"- 测试集: {len(test_source)} 个样本\")\n",
    "    \n",
    "    # 显示一些训练样本\n",
    "    print(\"\\n训练样本示例:\")\n",
    "    for i in range(min(3, len(train_source))):\n",
    "        print(f\"{i+1}. 源: {train_source[i]} -> 目标: {train_target[i]}\")\n",
    "    \n",
    "    # 创建数据集\n",
    "    train_dataset = CodeTranslationDataset(train_source, train_target, tokenizer, config['max_length'])\n",
    "    val_dataset = CodeTranslationDataset(val_source, val_target, tokenizer, config['max_length'])\n",
    "    \n",
    "    # 创建数据加载器\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config['batch_size'])\n",
    "    \n",
    "    # 初始化模型\n",
    "    model = ImprovedCodeBERTTranslator(\n",
    "        model_name=config['model_name'],\n",
    "        vocab_size=tokenizer.vocab_size,\n",
    "        max_length=config['max_length']\n",
    "    )\n",
    "    \n",
    "    model_path = 'improved_codebert_translator.pth'\n",
    "    \n",
    "    # 训练模型\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"使用设备: {device}\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "    print(\"\\n训练模型...\")\n",
    "    train_losses, val_losses = train_model(\n",
    "        model, train_loader, val_loader,\n",
    "        epochs=config['epochs'],\n",
    "        learning_rate=config['learning_rate']\n",
    "    )\n",
    "    \n",
    "    # 保存模型\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(f\"模型已保存到 '{model_path}'\")\n",
    "    \n",
    "    # 测试模型\n",
    "    print(\"\\n测试模型...\")\n",
    "    test_samples = list(zip(test_source, test_target))\n",
    "    print(f\"测试样本数量: {len(test_samples)}\")\n",
    "    \n",
    "    results, accuracy = evaluate_model(model, tokenizer, test_samples, device)\n",
    "    \n",
    "    # 演示翻译功能\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    demo_samples = [\n",
    "        'System.out.println(\"hello world\");',\n",
    "        'int x = 5;',\n",
    "        'if (x > 0) { return true; }'\n",
    "    ]\n",
    "    \n",
    "    for i, source_code in enumerate(demo_samples):\n",
    "        print(f\"\\n演示 {i+1}:\")\n",
    "        translated = translate_code(model, tokenizer, source_code, device)\n",
    "        print(f\"输入: {source_code}\")\n",
    "        print(f\"输出: {translated}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30d97dee-1bac-44fe-88f1-035b91c99841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "配置参数: {'batch_size': 4, 'max_length': 64, 'learning_rate': 3e-05, 'epochs': 15, 'model_name': 'microsoft/codebert-base'}\n",
      "Tokenizer词汇表大小: 50265\n",
      "数据集大小:\n",
      "- 训练集: 7 个样本\n",
      "- 验证集: 2 个样本\n",
      "- 测试集: 1 个样本\n",
      "\n",
      "训练样本示例:\n",
      "1. 源: System.out.println(\"hello\"); -> 目标: print(\"hello\")\n",
      "2. 源: int x = 10; -> 目标: x = 10\n",
      "3. 源: if (x > 5) { return true; } -> 目标: if x > 5: return True\n",
      "模型初始化完成: hidden_size=768, vocab_size=50265\n",
      "使用设备: cuda\n",
      "\n",
      "训练模型...\n",
      "训练样本数: 7\n",
      "验证样本数: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15 [Train]: 100%|██████████████████████████████████████| 2/2 [00:00<00:00,  2.72it/s, loss=10.7571, acc=0.0000]\n",
      "Epoch 1/15 [Val]: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 70.90it/s, loss=10.7379, acc=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 10.7881, Train Acc: 0.0000, Val Loss: 10.7379, Val Acc: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/15 [Train]: 100%|██████████████████████████████████████| 2/2 [00:00<00:00,  8.50it/s, loss=10.6854, acc=0.0000]\n",
      "Epoch 2/15 [Val]: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 43.16it/s, loss=10.6269, acc=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss: 10.7025, Train Acc: 0.0000, Val Loss: 10.6269, Val Acc: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/15 [Train]: 100%|██████████████████████████████████████| 2/2 [00:00<00:00,  8.54it/s, loss=10.5660, acc=0.0000]\n",
      "Epoch 3/15 [Val]: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 68.97it/s, loss=10.5077, acc=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss: 10.5813, Train Acc: 0.0000, Val Loss: 10.5077, Val Acc: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/15 [Train]: 100%|██████████████████████████████████████| 2/2 [00:00<00:00,  8.69it/s, loss=10.5454, acc=0.0000]\n",
      "Epoch 4/15 [Val]: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 77.16it/s, loss=10.3760, acc=1.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss: 10.5662, Train Acc: 0.0000, Val Loss: 10.3760, Val Acc: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/15 [Train]: 100%|██████████████████████████████████████| 2/2 [00:00<00:00,  8.28it/s, loss=10.3306, acc=0.0000]\n",
      "Epoch 5/15 [Val]: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 46.18it/s, loss=10.2470, acc=1.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss: 10.3852, Train Acc: 0.2857, Val Loss: 10.2470, Val Acc: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/15 [Train]: 100%|██████████████████████████████████████| 2/2 [00:00<00:00,  8.24it/s, loss=10.2439, acc=0.6667]\n",
      "Epoch 6/15 [Val]: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 45.84it/s, loss=10.1310, acc=1.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Loss: 10.2341, Train Acc: 0.7143, Val Loss: 10.1310, Val Acc: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/15 [Train]: 100%|██████████████████████████████████████| 2/2 [00:00<00:00,  8.57it/s, loss=10.1017, acc=0.6667]\n",
      "Epoch 7/15 [Val]: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 59.35it/s, loss=10.0340, acc=1.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Loss: 10.1911, Train Acc: 0.8571, Val Loss: 10.0340, Val Acc: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/15 [Train]: 100%|██████████████████████████████████████| 2/2 [00:00<00:00,  8.11it/s, loss=10.0546, acc=1.0000]\n",
      "Epoch 8/15 [Val]: 100%|█████████████████████████████████████████| 1/1 [00:00<00:00, 61.04it/s, loss=9.9780, acc=1.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Loss: 10.0955, Train Acc: 1.0000, Val Loss: 9.9780, Val Acc: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/15 [Train]: 100%|██████████████████████████████████████| 2/2 [00:00<00:00,  8.27it/s, loss=10.1757, acc=1.0000]\n",
      "Epoch 9/15 [Val]: 100%|█████████████████████████████████████████| 1/1 [00:00<00:00, 50.08it/s, loss=9.9230, acc=1.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train Loss: 10.0940, Train Acc: 1.0000, Val Loss: 9.9230, Val Acc: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/15 [Train]: 100%|█████████████████████████████████████| 2/2 [00:00<00:00,  8.35it/s, loss=10.0790, acc=1.0000]\n",
      "Epoch 10/15 [Val]: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 38.25it/s, loss=9.8239, acc=1.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Loss: 9.9760, Train Acc: 1.0000, Val Loss: 9.8239, Val Acc: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/15 [Train]: 100%|██████████████████████████████████████| 2/2 [00:00<00:00,  8.59it/s, loss=9.7453, acc=1.0000]\n",
      "Epoch 11/15 [Val]: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 58.81it/s, loss=9.6538, acc=1.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Train Loss: 9.8289, Train Acc: 1.0000, Val Loss: 9.6538, Val Acc: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/15 [Train]: 100%|██████████████████████████████████████| 2/2 [00:00<00:00,  8.42it/s, loss=9.5996, acc=1.0000]\n",
      "Epoch 12/15 [Val]: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 56.70it/s, loss=9.5228, acc=1.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: Train Loss: 9.6710, Train Acc: 1.0000, Val Loss: 9.5228, Val Acc: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/15 [Train]: 100%|██████████████████████████████████████| 2/2 [00:00<00:00,  8.49it/s, loss=9.5727, acc=1.0000]\n",
      "Epoch 13/15 [Val]: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 60.32it/s, loss=9.4606, acc=1.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: Train Loss: 9.6198, Train Acc: 1.0000, Val Loss: 9.4606, Val Acc: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/15 [Train]: 100%|██████████████████████████████████████| 2/2 [00:00<00:00,  8.46it/s, loss=9.4757, acc=1.0000]\n",
      "Epoch 14/15 [Val]: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 57.46it/s, loss=9.3467, acc=1.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: Train Loss: 9.5400, Train Acc: 1.0000, Val Loss: 9.3467, Val Acc: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/15 [Train]: 100%|██████████████████████████████████████| 2/2 [00:00<00:00,  8.41it/s, loss=9.5312, acc=1.0000]\n",
      "Epoch 15/15 [Val]: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 50.96it/s, loss=9.2151, acc=1.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: Train Loss: 9.5228, Train Acc: 1.0000, Val Loss: 9.2151, Val Acc: 1.0000\n",
      "模型已保存到 'improved_codebert_translator.pth'\n",
      "\n",
      "测试模型...\n",
      "测试样本数量: 1\n",
      "\n",
      "--- 测试样本 1/1 ---\n",
      "Top-5 预测:\n",
      "  1. '' (ID: 0, 概率: 0.0001)\n",
      "  2. 'minute' (ID: 4530, 概率: 0.0000)\n",
      "  3. 'Orange' (ID: 37264, 概率: 0.0000)\n",
      "  4. ' wow' (ID: 26388, 概率: 0.0000)\n",
      "  5. 'Techn' (ID: 40529, 概率: 0.0000)\n",
      "最终翻译结果: ''\n",
      "期望结果: 'arr = [1,2,3]'\n",
      "是否匹配: False\n",
      "\n",
      "精确匹配准确率: 0.0000 (0/1)\n",
      "\n",
      "============================================================\n",
      "============================================================\n",
      "\n",
      "演示 1:\n",
      "Top-5 预测:\n",
      "  1. '' (ID: 0, 概率: 0.0001)\n",
      "  2. 'minute' (ID: 4530, 概率: 0.0000)\n",
      "  3. 'Orange' (ID: 37264, 概率: 0.0000)\n",
      "  4. ' wow' (ID: 26388, 概率: 0.0000)\n",
      "  5. ' OFFIC' (ID: 38872, 概率: 0.0000)\n",
      "最终翻译结果: ''\n",
      "输入: System.out.println(\"hello world\");\n",
      "输出: \n",
      "\n",
      "演示 2:\n",
      "Top-5 预测:\n",
      "  1. '' (ID: 0, 概率: 0.0001)\n",
      "  2. 'minute' (ID: 4530, 概率: 0.0000)\n",
      "  3. 'Orange' (ID: 37264, 概率: 0.0000)\n",
      "  4. ' wow' (ID: 26388, 概率: 0.0000)\n",
      "  5. ' OFFIC' (ID: 38872, 概率: 0.0000)\n",
      "最终翻译结果: ''\n",
      "输入: int x = 5;\n",
      "输出: \n",
      "\n",
      "演示 3:\n",
      "Top-5 预测:\n",
      "  1. '' (ID: 0, 概率: 0.0001)\n",
      "  2. 'minute' (ID: 4530, 概率: 0.0000)\n",
      "  3. 'Orange' (ID: 37264, 概率: 0.0000)\n",
      "  4. ' OFFIC' (ID: 38872, 概率: 0.0000)\n",
      "  5. ' wow' (ID: 26388, 概率: 0.0000)\n",
      "最终翻译结果: ''\n",
      "输入: if (x > 0) { return true; }\n",
      "输出: \n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad6b129-d512-4d3d-8e24-999b7376d2f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
