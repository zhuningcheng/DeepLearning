{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "999c8f0f-5ba3-409c-9f65-5647ada32378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里使用codebert实现了java到python的代码翻译功能使用的是预训练codebert模型\n",
    "#模型是出自于微软的microsoft/codebert-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bb62f22-6564-4cbc-a76f-5f31c30fe8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "325b763d-af22-4957-96e2-bc45445287f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 任务3：数据集类定义\n",
    "# 运行这个单元格定义数据加载器\n",
    "\n",
    "class CodeTranslationDataset(Dataset):\n",
    "    def __init__(self, source_codes, target_codes, tokenizer, max_length=64):\n",
    "        self.source_codes = source_codes\n",
    "        self.target_codes = target_codes\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.source_codes)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        source_code = str(self.source_codes[idx])\n",
    "        target_code = str(self.target_codes[idx])\n",
    "        \n",
    "        # Tokenize source code\n",
    "        source_encoding = self.tokenizer(\n",
    "            source_code,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Tokenize target code\n",
    "        target_encoding = self.tokenizer(\n",
    "            target_code,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': source_encoding['input_ids'].flatten(),\n",
    "            'attention_mask': source_encoding['attention_mask'].flatten(),\n",
    "            'labels': target_encoding['input_ids'].flatten()\n",
    "        }\n",
    "\n",
    "def load_data(file_path):\n",
    "    \n",
    "    df = pd.read_csv(file_path)\n",
    "    return df['source_code'].tolist(), df['target_code'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed07052f-dc26-42af-a75a-2ab91e406f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建CodeBert的模型\n",
    "\n",
    "class ImprovedCodeBERTTranslator(nn.Module):\n",
    "    def __init__(self, model_name='microsoft/codebert-base', vocab_size=50265, max_length=64):\n",
    "        super(ImprovedCodeBERTTranslator, self).__init__()\n",
    "        \n",
    "        # 加载预训练的CodeBERT模型\n",
    "        self.codebert = RobertaModel.from_pretrained(model_name)\n",
    "        self.hidden_size = self.codebert.config.hidden_size\n",
    "        \n",
    "        # 解码器 \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.hidden_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, vocab_size)\n",
    "        )\n",
    "        \n",
    "        print(f\"模型初始化完成: hidden_size={self.hidden_size}, vocab_size={vocab_size}\")\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        # 编码器前向传播\n",
    "        encoder_outputs = self.codebert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "        \n",
    "        # 使用[CLS] token的输出作为序列表示\n",
    "        cls_output = encoder_outputs.last_hidden_state[:, 0, :]\n",
    "        \n",
    "        # 解码\n",
    "        logits = self.decoder(cls_output)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a0e15df-8bca-445b-bc1c-3fb341a82910",
   "metadata": {},
   "outputs": [],
   "source": [
    "def improved_train_model(model, train_loader, val_loader, epochs=10, learning_rate=5e-5):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    \n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=1)  # 忽略padding token\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    print(f\"训练样本数: {len(train_loader.dataset)}\")\n",
    "    print(f\"验证样本数: {len(val_loader.dataset)}\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        # 训练阶段\n",
    "        train_pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs} [Train]')\n",
    "        for batch_idx, batch in enumerate(train_pbar):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask, labels)\n",
    "            \n",
    "            # 计算准确率 - 预测目标序列的第一个token\n",
    "            preds = torch.argmax(outputs, dim=-1)\n",
    "            targets = labels[:, 0]  # 目标序列的第一个token\n",
    "            \n",
    "            correct = (preds == targets).sum().item()\n",
    "            total = targets.size(0)\n",
    "            train_correct += correct\n",
    "            train_total += total\n",
    "            \n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_train_loss += loss.item()\n",
    "            train_pbar.set_postfix({\n",
    "                'loss': f'{loss.item():.4f}',\n",
    "                'acc': f'{correct/total:.4f}' if total > 0 else '0.0000'\n",
    "            })\n",
    "        \n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        train_accuracy = train_correct / train_total if train_total > 0 else 0\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        # 验证阶段\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            val_pbar = tqdm(val_loader, desc=f'Epoch {epoch+1}/{epochs} [Val]')\n",
    "            for batch in val_pbar:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                \n",
    "                outputs = model(input_ids, attention_mask, labels)\n",
    "                preds = torch.argmax(outputs, dim=-1)\n",
    "                targets = labels[:, 0]\n",
    "                \n",
    "                correct = (preds == targets).sum().item()\n",
    "                total = targets.size(0)\n",
    "                val_correct += correct\n",
    "                val_total += total\n",
    "                \n",
    "                loss = criterion(outputs, targets)\n",
    "                total_val_loss += loss.item()\n",
    "                \n",
    "                val_pbar.set_postfix({\n",
    "                    'loss': f'{loss.item():.4f}',\n",
    "                    'acc': f'{correct/total:.4f}' if total > 0 else '0.0000'\n",
    "                })\n",
    "        \n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        val_accuracy = val_correct / val_total if val_total > 0 else 0\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}: Train Loss: {avg_train_loss:.4f}, Train Acc: {train_accuracy:.4f}, Val Loss: {avg_val_loss:.4f}, Val Acc: {val_accuracy:.4f}')\n",
    "    \n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c066ef5e-f4d8-47f1-872f-307d4ff9bfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_code(model, tokenizer, source_code, device):\n",
    "        model.eval()\n",
    "        \n",
    "        # 编码源代码\n",
    "        encoding = tokenizer(\n",
    "            source_code,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=64,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        input_ids = encoding['input_ids'].to(device)\n",
    "        attention_mask = encoding['attention_mask'].to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            \n",
    "            # 获取预测结果\n",
    "            predicted_token_id = torch.argmax(outputs, dim=-1).item()\n",
    "            \n",
    "            # 解码预测的token\n",
    "            predicted_token = tokenizer.decode([predicted_token_id], skip_special_tokens=True)\n",
    "            \n",
    "            # 获取top-k预测结果\n",
    "            top_k = 5\n",
    "            probs = torch.softmax(outputs, dim=-1)\n",
    "            top_probs, top_indices = torch.topk(probs, top_k)\n",
    "            \n",
    "            print(f\"Top-{top_k} 预测:\")\n",
    "            for i in range(top_k):\n",
    "                token_id = top_indices[0, i].item()\n",
    "                token = tokenizer.decode([token_id], skip_special_tokens=True)\n",
    "                prob = top_probs[0, i].item()\n",
    "                print(f\"  {i+1}. '{token}' (ID: {token_id}, 概率: {prob:.4f})\")\n",
    "        \n",
    "        print(f\"最终翻译结果: '{predicted_token}'\")\n",
    "        return predicted_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38ea84c9-d5d3-4c09-832c-91ed9a006154",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, tokenizer, test_samples, device):\n",
    "    results = []\n",
    "    \n",
    "    \n",
    "    for i, (source, target) in enumerate(test_samples):\n",
    "        print(f\"\\n--- 测试样本 {i+1}/{len(test_samples)} ---\")\n",
    "        translated = improved_translate_code(model, tokenizer, source, device)\n",
    "        \n",
    "        # 检查翻译结果\n",
    "        exact_match = translated.strip() == target.strip()\n",
    "        \n",
    "        results.append({\n",
    "            'source': source,\n",
    "            'target': target,\n",
    "            'translated': translated,\n",
    "            'exact_match': exact_match\n",
    "        })\n",
    "        \n",
    "        print(f\"期望结果: '{target}'\")\n",
    "        print(f\"是否匹配: {exact_match}\")\n",
    "    \n",
    "    exact_match_accuracy = sum([r['exact_match'] for r in results]) / len(results)\n",
    "    print(f\"\\n精确匹配准确率: {exact_match_accuracy:.4f} ({sum([r['exact_match'] for r in results])}/{len(results)})\")\n",
    "    \n",
    "    return results, exact_match_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8f26eec-b329-4690-9139-990e63b1c612",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # 配置参数\n",
    "    config = {\n",
    "        'batch_size': 4,\n",
    "        'max_length': 64,  # 序列长度\n",
    "        'learning_rate': 3e-5,  # 学习率\n",
    "        'epochs': 15,      # 训练轮数\n",
    "        'model_name': 'microsoft/codebert-base'\n",
    "    }\n",
    "    \n",
    "    print(f\"配置参数: {config}\")\n",
    "    \n",
    "    # 加载tokenizer\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(config['model_name'])\n",
    "    print(f\"Tokenizer词汇表大小: {tokenizer.vocab_size}\")\n",
    "    \n",
    "    # 加载数据\n",
    "    train_source, train_target = load_data('data/train.csv')\n",
    "    val_source, val_target = load_data('data/val.csv')\n",
    "    test_source, test_target = load_data('data/test.csv')\n",
    "    \n",
    "    print(f\"数据集大小:\")\n",
    "    print(f\"- 训练集: {len(train_source)} 个样本\")\n",
    "    print(f\"- 验证集: {len(val_source)} 个样本\")\n",
    "    print(f\"- 测试集: {len(test_source)} 个样本\")\n",
    "    \n",
    "    # 显示一些训练样本\n",
    "    print(\"\\n训练样本示例:\")\n",
    "    for i in range(min(3, len(train_source))):\n",
    "        print(f\"{i+1}. 源: {train_source[i]} -> 目标: {train_target[i]}\")\n",
    "    \n",
    "    # 创建数据集\n",
    "    train_dataset = CodeTranslationDataset(train_source, train_target, tokenizer, config['max_length'])\n",
    "    val_dataset = CodeTranslationDataset(val_source, val_target, tokenizer, config['max_length'])\n",
    "    \n",
    "    # 创建数据加载器\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config['batch_size'])\n",
    "    \n",
    "    # 初始化模型\n",
    "    model = ImprovedCodeBERTTranslator(\n",
    "        model_name=config['model_name'],\n",
    "        vocab_size=tokenizer.vocab_size,\n",
    "        max_length=config['max_length']\n",
    "    )\n",
    "    \n",
    "    model_path = 'improved_codebert_translator.pth'\n",
    "    \n",
    "    # 训练模型\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"使用设备: {device}\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "    print(\"\\n训练模型...\")\n",
    "    train_losses, val_losses = train_model(\n",
    "        model, train_loader, val_loader,\n",
    "        epochs=config['epochs'],\n",
    "        learning_rate=config['learning_rate']\n",
    "    )\n",
    "    \n",
    "    # 保存模型\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(f\"模型已保存到 '{model_path}'\")\n",
    "    \n",
    "    # 测试模型\n",
    "    print(\"\\n测试模型...\")\n",
    "    test_samples = list(zip(test_source, test_target))\n",
    "    print(f\"测试样本数量: {len(test_samples)}\")\n",
    "    \n",
    "    results, accuracy = evaluate_model(model, tokenizer, test_samples, device)\n",
    "    \n",
    "    # 演示翻译功能\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    demo_samples = [\n",
    "        'System.out.println(\"hello world\");',\n",
    "        'int x = 5;',\n",
    "        'if (x > 0) { return true; }'\n",
    "    ]\n",
    "    \n",
    "    for i, source_code in enumerate(demo_samples):\n",
    "        print(f\"\\n演示 {i+1}:\")\n",
    "        translated = translate_code(model, tokenizer, source_code, device)\n",
    "        print(f\"输入: {source_code}\")\n",
    "        print(f\"输出: {translated}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30d97dee-1bac-44fe-88f1-035b91c99841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "配置参数: {'batch_size': 4, 'max_length': 64, 'learning_rate': 3e-05, 'epochs': 15, 'model_name': 'microsoft/codebert-base'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /microsoft/codebert-base/resolve/main/tokenizer_config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001C982DB14C0>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 849c42be-e2f2-4401-97d1-f883402d2d82)')' thrown while requesting HEAD https://huggingface.co/microsoft/codebert-base/resolve/main/tokenizer_config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /microsoft/codebert-base/resolve/main/tokenizer_config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x000001C98476CE90>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 19c94bec-814b-4150-bbde-74010afd2a3c)')' thrown while requesting HEAD https://huggingface.co/microsoft/codebert-base/resolve/main/tokenizer_config.json\n",
      "Retrying in 2s [Retry 2/5].\n",
      "'(ProtocolError('Connection aborted.', ConnectionResetError(10054, '远程主机强迫关闭了一个现有的连接。', None, 10054, None)), '(Request ID: 0737e242-3dbf-4111-81d4-4e02fe45f0c1)')' thrown while requesting HEAD https://huggingface.co/microsoft/codebert-base/resolve/main/tokenizer_config.json\n",
      "Retrying in 4s [Retry 3/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer词汇表大小: 50265\n",
      "数据集大小:\n",
      "- 训练集: 7 个样本\n",
      "- 验证集: 2 个样本\n",
      "- 测试集: 1 个样本\n",
      "\n",
      "训练样本示例:\n",
      "1. 源: System.out.println(\"hello\"); -> 目标: print(\"hello\")\n",
      "2. 源: int x = 10; -> 目标: x = 10\n",
      "3. 源: if (x > 5) { return true; } -> 目标: if x > 5: return True\n",
      "模型初始化完成: hidden_size=768, vocab_size=50265\n",
      "使用设备: cuda\n",
      "\n",
      "训练模型...\n",
      "训练样本数: 7\n",
      "验证样本数: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15 [Train]: 100%|██████████████████████████████████████| 2/2 [00:01<00:00,  1.85it/s, loss=10.8950, acc=0.0000]\n",
      "Epoch 1/15 [Val]: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 45.24it/s, loss=10.8274, acc=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 10.9249, Train Acc: 0.0000, Val Loss: 10.8274, Val Acc: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/15 [Train]: 100%|██████████████████████████████████████| 2/2 [00:00<00:00,  8.40it/s, loss=10.8548, acc=0.0000]\n",
      "Epoch 2/15 [Val]: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 62.37it/s, loss=10.7458, acc=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss: 10.8309, Train Acc: 0.0000, Val Loss: 10.7458, Val Acc: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/15 [Train]: 100%|██████████████████████████████████████| 2/2 [00:00<00:00,  8.22it/s, loss=10.6796, acc=0.0000]\n",
      "Epoch 3/15 [Val]: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 67.58it/s, loss=10.6510, acc=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss: 10.7143, Train Acc: 0.0000, Val Loss: 10.6510, Val Acc: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/15 [Train]: 100%|██████████████████████████████████████| 2/2 [00:00<00:00,  8.61it/s, loss=10.6226, acc=0.0000]\n",
      "Epoch 4/15 [Val]: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 47.31it/s, loss=10.5454, acc=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss: 10.6677, Train Acc: 0.0000, Val Loss: 10.5454, Val Acc: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/15 [Train]: 100%|██████████████████████████████████████| 2/2 [00:00<00:00,  8.27it/s, loss=10.5363, acc=0.0000]\n",
      "Epoch 5/15 [Val]: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 66.44it/s, loss=10.4293, acc=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss: 10.6019, Train Acc: 0.0000, Val Loss: 10.4293, Val Acc: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/15 [Train]: 100%|██████████████████████████████████████| 2/2 [00:00<00:00,  8.52it/s, loss=10.3971, acc=0.0000]\n",
      "Epoch 6/15 [Val]: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 72.57it/s, loss=10.3031, acc=1.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Loss: 10.4226, Train Acc: 0.0000, Val Loss: 10.3031, Val Acc: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/15 [Train]: 100%|██████████████████████████████████████| 2/2 [00:00<00:00,  8.08it/s, loss=10.2346, acc=0.3333]\n",
      "Epoch 7/15 [Val]: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 70.91it/s, loss=10.1831, acc=1.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Loss: 10.2679, Train Acc: 0.4286, Val Loss: 10.1831, Val Acc: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/15 [Train]: 100%|██████████████████████████████████████| 2/2 [00:00<00:00,  8.31it/s, loss=10.3554, acc=0.0000]\n",
      "Epoch 8/15 [Val]: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 53.44it/s, loss=10.0742, acc=1.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Loss: 10.2749, Train Acc: 0.2857, Val Loss: 10.0742, Val Acc: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/15 [Train]: 100%|██████████████████████████████████████| 2/2 [00:00<00:00,  8.10it/s, loss=10.1604, acc=0.3333]\n",
      "Epoch 9/15 [Val]: 100%|█████████████████████████████████████████| 1/1 [00:00<00:00, 71.84it/s, loss=9.9795, acc=1.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train Loss: 10.1371, Train Acc: 0.5714, Val Loss: 9.9795, Val Acc: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/15 [Train]: 100%|██████████████████████████████████████| 2/2 [00:00<00:00,  8.29it/s, loss=9.9988, acc=1.0000]\n",
      "Epoch 10/15 [Val]: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 72.28it/s, loss=9.8755, acc=1.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Loss: 10.0467, Train Acc: 1.0000, Val Loss: 9.8755, Val Acc: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/15 [Train]: 100%|██████████████████████████████████████| 2/2 [00:00<00:00,  8.23it/s, loss=9.8719, acc=1.0000]\n",
      "Epoch 11/15 [Val]: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 53.68it/s, loss=9.7682, acc=1.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Train Loss: 9.9612, Train Acc: 0.8571, Val Loss: 9.7682, Val Acc: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/15 [Train]: 100%|██████████████████████████████████████| 2/2 [00:00<00:00,  8.02it/s, loss=9.8727, acc=1.0000]\n",
      "Epoch 12/15 [Val]: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 46.32it/s, loss=9.6608, acc=1.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: Train Loss: 9.8630, Train Acc: 1.0000, Val Loss: 9.6608, Val Acc: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/15 [Train]: 100%|██████████████████████████████████████| 2/2 [00:00<00:00,  8.46it/s, loss=9.6927, acc=1.0000]\n",
      "Epoch 13/15 [Val]: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 45.64it/s, loss=9.5167, acc=1.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: Train Loss: 9.7430, Train Acc: 1.0000, Val Loss: 9.5167, Val Acc: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/15 [Train]: 100%|██████████████████████████████████████| 2/2 [00:00<00:00,  8.32it/s, loss=9.5922, acc=1.0000]\n",
      "Epoch 14/15 [Val]: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 64.58it/s, loss=9.3605, acc=1.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: Train Loss: 9.6096, Train Acc: 1.0000, Val Loss: 9.3605, Val Acc: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/15 [Train]: 100%|██████████████████████████████████████| 2/2 [00:00<00:00,  8.35it/s, loss=9.3771, acc=1.0000]\n",
      "Epoch 15/15 [Val]: 100%|████████████████████████████████████████| 1/1 [00:00<00:00, 65.38it/s, loss=9.2278, acc=1.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: Train Loss: 9.4621, Train Acc: 1.0000, Val Loss: 9.2278, Val Acc: 1.0000\n",
      "模型已保存到 'improved_codebert_translator.pth'\n",
      "\n",
      "测试模型...\n",
      "测试样本数量: 1\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'improved_evaluate_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m----> 2\u001b[0m     main()\n",
      "Cell \u001b[1;32mIn[8], line 70\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     67\u001b[0m test_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(test_source, test_target))\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m测试样本数量: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(test_samples)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 70\u001b[0m results, accuracy \u001b[38;5;241m=\u001b[39m improved_evaluate_model(model, tokenizer, test_samples, device)\n\u001b[0;32m     72\u001b[0m \u001b[38;5;66;03m# 演示翻译功能\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m60\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'improved_evaluate_model' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad6b129-d512-4d3d-8e24-999b7376d2f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
